{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "srNeFWy71nQ6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vq0hftPy1pbA"
   },
   "outputs": [],
   "source": [
    "# data loading tools\n",
    "  \n",
    "def add_features(df, data, features):\n",
    "    \"\"\"given dataframe, embedding vectors, and a dictionary of features for samples, create samples that contain both embeddings and linguistic features\"\"\"\n",
    "    \n",
    "    dataset = []\n",
    "    for i in tqdm_notebook(range(len(data))): \n",
    "\n",
    "      # empty tensor for words #\n",
    "        sample = data[i][1:]\n",
    "        idx = df.ID[i]\n",
    "        feat = torch.FloatTensor(features[idx])\n",
    "        new = (idx, feat, ) + sample\n",
    "\n",
    "        dataset.append(new)\n",
    "    \n",
    "    return dataset\n",
    "  \n",
    "  \n",
    "  \n",
    "def dataloader(data, batch_size, maxlen, emb_dim):\n",
    "    \"\"\"batch the vectorized data\"\"\"\n",
    "    \n",
    "    batches = []\n",
    "    \n",
    "    temp_data = deepcopy(data)\n",
    "    random.shuffle(temp_data)\n",
    "    \n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batches.append(temp_data[i:i + batch_size])\n",
    "    \n",
    "    dl = []\n",
    "    \n",
    "    for i in range(len(batches)):\n",
    "        batches[i] = sorted(batches[i], key=lambda x: x[3], reverse=True)\n",
    "        dl.append([])\n",
    "        \n",
    "        # IDs\n",
    "        dl[-1].append(torch.tensor([sample[0] for sample in batches[i]]))\n",
    "        \n",
    "        # linguistic features\n",
    "        dl[-1].append(torch.zeros(len(batches[i]), 77))\n",
    "        for j in range(len(batches[i])):\n",
    "            dl[-1][-1][j, :] = batches[i][j][1]\n",
    "        \n",
    "        # Embeddings\n",
    "        dl[-1].append(torch.zeros(len(batches[i]), min(maxlen, batches[i][0][3]), emb_dim))\n",
    "        for j in range(len(batches[i])):\n",
    "            dl[-1][-1][j, :min(maxlen, batches[i][j][3]), :] = batches[i][j][2][:min(maxlen, batches[i][j][3]), :]\n",
    "        dl[-1][-1] = dl[-1][-1].permute(1,0,2)\n",
    "        \n",
    "        # Lengths\n",
    "        dl[-1].append(torch.tensor([min(sample[3], maxlen) for sample in batches[i]]))\n",
    "        \n",
    "        # Labels\n",
    "        dl[-1].append(torch.tensor([sample[4] for sample in batches[i]]))\n",
    "        dl[-1].append(torch.tensor([sample[5] for sample in batches[i]]))\n",
    "        \n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJhd9HV711Uj"
   },
   "outputs": [],
   "source": [
    "# the joint model, composed of a neural and a linguistic sub-model\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_type, n_hidden, n_layers, pool_type, n_out, direction, dropout, emb_dropout):\n",
    "        super().__init__()\n",
    "        self.embedding_dim, self.hidden_type, self.n_hidden, self.n_layers, self.pool_type, self.n_out, self.direction, self.dropout, self.emb_dropout = emb_dim, hidden_type, n_hidden, n_layers, pool_type, n_out, direction, dropout, emb_dropout\n",
    "        self.embedding_dropout = nn.Dropout(p=self.emb_dropout)\n",
    "        if type(self.pool_type) != list:\n",
    "            self.pool_type = [self.pool_type]\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        if use_features:\n",
    "            self.linear = nn.Linear(num_features, num_features)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        if self.hidden_type == 'GRU-CNN':\n",
    "            self.number_of_hidden = 64\n",
    "            self.number_of_layers = 1\n",
    "          \n",
    "            self.hidden1 = nn.GRU(input_size=self.embedding_dim, hidden_size=self.number_of_hidden, num_layers=self.number_of_layers, dropout = self.dropout, bidirectional = True)\n",
    "            self.hidden = []\n",
    "            for i in range(len(self.n_hidden)):\n",
    "                self.hidden.append(nn.Conv2d(in_channels=1, out_channels=self.n_hidden[i], kernel_size=(self.n_layers[i], 2*self.number_of_hidden), padding=(self.n_layers[i]//2, 0)))\n",
    "            self.hidden = nn.ModuleList(self.hidden)\n",
    "            self.attn = []\n",
    "            for i in range(len(self.n_hidden)):\n",
    "                self.attn.append(nn.Linear(self.n_hidden[i], 1))\n",
    "            self.attn = nn.ModuleList(self.attn)\n",
    "            if 'max' in pool_type:\n",
    "                if use_features:\n",
    "                    self.out = nn.Linear((len(self.pool_type)+1)*sum(self.n_hidden)+num_features, self.n_out)\n",
    "                else:\n",
    "                    self.out = nn.Linear((len(self.pool_type)+1)*sum(self.n_hidden), self.n_out)\n",
    "            else:\n",
    "                if use_features:\n",
    "                    self.out = nn.Linear(len(self.pool_type)*sum(self.n_hidden)+num_features, self.n_out)\n",
    "                else:\n",
    "                    self.out = nn.Linear(len(self.pool_type)*sum(self.n_hidden), self.n_out)\n",
    "\n",
    "        elif self.hidden_type == 'transformer':\n",
    "            self.hidden = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=n_hidden, nhead=6), num_layers=n_layers)\n",
    "            self.attn = nn.Linear(self.n_hidden, 1)\n",
    "            if 'max' in pool_type:\n",
    "                if use_features:\n",
    "                    self.out = nn.Linear((len(self.pool_type)+1) * self.n_hidden+num_features, self.n_out)\n",
    "                else:\n",
    "                    self.out = nn.Linear((len(self.pool_type)+1) * self.n_hidden, self.n_out)\n",
    "            else:\n",
    "                if use_features:\n",
    "                    self.out = nn.Linear(len(self.pool_type) * self.n_hidden+num_features, self.n_out)\n",
    "                else:\n",
    "                    self.out = nn.Linear(len(self.pool_type) * self.n_hidden, self.n_out)\n",
    "        \n",
    "        elif self.hidden_type == 'CNN':\n",
    "            self.hidden = []\n",
    "            for i in range(len(self.n_hidden)):\n",
    "                self.hidden.append(nn.Conv2d(in_channels=1, out_channels=self.n_hidden[i], kernel_size=(self.n_layers[i], self.embedding_dim), padding=(self.n_layers[i]//2, 0)))\n",
    "            self.hidden = nn.ModuleList(self.hidden)\n",
    "            self.attn = []\n",
    "            for i in range(len(self.n_hidden)):\n",
    "                self.attn.append(nn.Linear(self.n_hidden[i], 1))\n",
    "            self.attn = nn.ModuleList(self.attn)\n",
    "            if 'max' in pool_type:\n",
    "                if use_features:\n",
    "                    self.out = nn.Linear((len(self.pool_type)+1)*sum(self.n_hidden)+num_features, self.n_out)\n",
    "                else:\n",
    "                    self.out = nn.Linear((len(self.pool_type)+1)*sum(self.n_hidden), self.n_out)\n",
    "            else:\n",
    "                if use_features:\n",
    "                    self.out = nn.Linear(len(self.pool_type)*sum(self.n_hidden)+num_features, self.n_out)\n",
    "                else:\n",
    "                    self.out = nn.Linear(len(self.pool_type)*sum(self.n_hidden), self.n_out)\n",
    "  \n",
    "        elif self.direction == 'unidirectional':\n",
    "            if self.hidden_type == 'RNN':\n",
    "                self.hidden = nn.RNN(input_size=self.embedding_dim, hidden_size=self.n_hidden, num_layers=self.n_layers, dropout = self.dropout)\n",
    "            elif self.hidden_type == 'GRU':\n",
    "                self.hidden = nn.GRU(input_size=self.embedding_dim, hidden_size=self.n_hidden, num_layers=self.n_layers, dropout = self.dropout)\n",
    "            elif self.hidden_type == 'LSTM':\n",
    "                self.hidden = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.n_hidden, num_layers=self.n_layers, dropout = self.dropout)\n",
    "            self.attn = nn.Linear(self.n_hidden, 1)\n",
    "            if 'max' in pool_type:\n",
    "                if use_features:\n",
    "                    self.out = nn.Linear((len(self.pool_type)+1) * self.n_hidden+num_features, self.n_out)\n",
    "                else:\n",
    "                    self.out = nn.Linear((len(self.pool_type)+1) * self.n_hidden, self.n_out)\n",
    "            else:\n",
    "                if use_features:\n",
    "                    self.out = nn.Linear(len(self.pool_type) * self.n_hidden+num_features, self.n_out)\n",
    "                else:\n",
    "                    self.out = nn.Linear(len(self.pool_type) * self.n_hidden, self.n_out)\n",
    "\n",
    "        elif self.direction == 'bidirectional':\n",
    "            if self.hidden_type == 'RNN':\n",
    "                self.hidden = nn.RNN(input_size=self.embedding_dim, hidden_size=self.n_hidden, num_layers=self.n_layers, dropout = self.dropout, bidirectional = True)\n",
    "            elif self.hidden_type == 'GRU':\n",
    "                self.hidden = nn.GRU(input_size=self.embedding_dim, hidden_size=self.n_hidden, num_layers=self.n_layers, dropout = self.dropout, bidirectional = True)\n",
    "            elif self.hidden_type == 'LSTM':\n",
    "                self.hidden = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.n_hidden, num_layers=self.n_layers, dropout = self.dropout, bidirectional = True)\n",
    "                         \n",
    "            self.attn = nn.Linear(2*self.n_hidden, 1)\n",
    "            if 'max' in pool_type:\n",
    "                if use_features:\n",
    "                    self.out = nn.Linear(2 * (len(self.pool_type)+1) * self.n_hidden+num_features, self.n_out)\n",
    "                else:\n",
    "                    self.out = nn.Linear(2 * (len(self.pool_type)+1) * self.n_hidden, self.n_out)\n",
    "            else:\n",
    "                if use_features:\n",
    "                    self.out = nn.Linear(2 * len(self.pool_type) * self.n_hidden+num_features, self.n_out)\n",
    "                else:\n",
    "                    self.out = nn.Linear(2 * len(self.pool_type) * self.n_hidden, self.n_out)\n",
    "    \n",
    "    def deactivate_dropout(self):\n",
    "        self.embedding_dropout = nn.Dropout(p=0)\n",
    "        \n",
    "    def activate_dropout(self):\n",
    "        self.embedding_dropout = nn.Dropout(p=self.emb_dropout)\n",
    "    \n",
    "    def init_hidden(self, batch_size, gpu = False):\n",
    "        \n",
    "        if self.hidden_type == 'GRU-CNN':\n",
    "            if gpu:\n",
    "                return Variable(torch.zeros(2 * self.number_of_layers,batch_size,self.number_of_hidden).cuda())\n",
    "            else:\n",
    "                return Variable(torch.zeros(2 * self.number_of_layers,batch_size,self.number_of_hidden))\n",
    "          \n",
    "        if self.direction == 'unidirectional':\n",
    "            if gpu:\n",
    "                return Variable(torch.zeros(self.n_layers,batch_size,self.n_hidden).cuda())\n",
    "            else:\n",
    "                return Variable(torch.zeros(self.n_layers,batch_size,self.n_hidden))\n",
    "        elif self.direction == 'bidirectional':\n",
    "            if gpu:\n",
    "                return Variable(torch.zeros(2 * self.n_layers,batch_size,self.n_hidden).cuda())\n",
    "            else:\n",
    "                return Variable(torch.zeros(2 * self.n_layers,batch_size,self.n_hidden))\n",
    "    \n",
    "    def attention(self, hidden_out, lengths=None, n=None):\n",
    "        if n==None:\n",
    "            attn_out = self.attn(hidden_out)\n",
    "        else:\n",
    "            attn_out = self.attn[n](hidden_out)\n",
    "        if lengths is not None:\n",
    "            mask = torch.arange(torch.tensor(lengths[0]))[None, :] >= torch.tensor(lengths[:, None])\n",
    "            attn_out[mask.permute(1,0)] = float('-inf')\n",
    "        attn_weights = F.softmax(attn_out, 0)\n",
    "        new_hidden_state = torch.sum(hidden_out * attn_weights, 0)\n",
    "        return new_hidden_state\n",
    "    \n",
    "    def np_attention(self, hidden_out, h):\n",
    "        hidden = h.squeeze(0)\n",
    "        attn_weights = torch.bmm(hidden_out, hidden.unsqueeze(2)).squeeze(2)\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        new_hidden_state = torch.bmm(hidden_out.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        return new_hidden_state\n",
    "    \n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3)  # (N, Co, W)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, feats, seq, lengths, gpu = False):\n",
    "        bs = seq.size(1) # batch size\n",
    "        embs = seq\n",
    "        \n",
    "        if self.hidden_type != 'CNN' and self.hidden_type != 'transformer':\n",
    "            embs = pack_padded_sequence(embs, lengths)\n",
    "            self.h = self.init_hidden(bs, gpu)\n",
    "            self.c = self.init_hidden(bs, gpu)\n",
    "            \n",
    "        if self.hidden_type == 'RNN':\n",
    "            hidden_out, self.h = self.hidden(embs, self.h)\n",
    "        elif self.hidden_type == 'GRU':\n",
    "            hidden_out, self.h = self.hidden(embs, self.h)\n",
    "        elif self.hidden_type == 'LSTM':\n",
    "            hidden_out, (self.h, self.c) = self.hidden(embs, (self.h, self.c))\n",
    "        elif self.hidden_type == 'CNN':\n",
    "            hidden_out = []\n",
    "            for cnn in self.hidden:\n",
    "                hidden_out.append(cnn(embs.permute(1, 0, 2).unsqueeze(1)).squeeze(-1).permute(2, 0, 1))\n",
    "        elif self.hidden_type == 'GRU-CNN':\n",
    "            h_out, self.h = self.hidden1(embs, self.h)\n",
    "            h_out, lengths = pad_packed_sequence(h_out)\n",
    "            hidden_out = []\n",
    "            for cnn in self.hidden:\n",
    "                hidden_out.append(cnn(h_out.permute(1, 0, 2).unsqueeze(1)).squeeze(-1).permute(2, 0, 1))\n",
    "            \n",
    "        if 'CNN' in self.hidden_type:\n",
    "            avg_pool, max_pool, attn_pool = [], [], []\n",
    "            for i in range(len(hidden_out)):\n",
    "                avg_pool.append(F.adaptive_avg_pool1d(hidden_out[i].permute(1,2,0),1).view(seq.size(1),-1))\n",
    "                max_pool.append(F.adaptive_max_pool1d(self.relu(torch.cat([hidden_out[i], -hidden_out[i]], dim=-1)).permute(1,2,0),1).view(seq.size(1),-1))\n",
    "                attn_pool.append(self.attention(hidden_out[i], n=i))\n",
    "                \n",
    "        elif self.hidden_type == 'transformer':\n",
    "            padding_mask = torch.arange(torch.tensor(lengths[0]))[None, :] >= torch.tensor(lengths[:, None])\n",
    "            if gpu:\n",
    "                padding_mask = padding_mask.cuda()\n",
    "            hidden_out = self.hidden(embs, src_key_padding_mask=padding_mask)\n",
    "            avg_pool = F.adaptive_avg_pool1d(hidden_out.permute(1,2,0),1).view(seq.size(1),-1)\n",
    "            max_pool = F.adaptive_max_pool1d(self.relu(torch.cat([hidden_out, -hidden_out], dim=-1)).permute(1,2,0),1).view(seq.size(1),-1)\n",
    "            attn_pool = self.attention(hidden_out, lengths=lengths)\n",
    "        \n",
    "        else:\n",
    "            hidden_out, lengths = pad_packed_sequence(hidden_out)\n",
    "        \n",
    "            if self.direction == 'unidirectional':\n",
    "                avg_pool = F.adaptive_avg_pool1d(hidden_out.permute(1,2,0),1).view(seq.size(1),-1)\n",
    "                max_pool = F.adaptive_max_pool1d(self.relu(torch.cat([hidden_out, -hidden_out], dim=-1)).permute(1,2,0),1).view(seq.size(1),-1)\n",
    "                np_attn_pool = self.np_attention(hidden_out.permute(1, 0, 2), hidden_out[-1].unsqueeze(0))\n",
    "                attn_pool = self.attention(hidden_out, lengths=lengths)\n",
    "                last_pool = hidden_out[-1]\n",
    "            elif self.direction == 'bidirectional':\n",
    "                avg_pool = F.adaptive_avg_pool1d(hidden_out.permute(1,2,0),1).view(seq.size(1),-1)\n",
    "                max_pool = F.adaptive_max_pool1d(self.relu(torch.cat([hidden_out, -hidden_out], dim=-1)).permute(1,2,0),1).view(seq.size(1),-1)\n",
    "                np_attn_pool = self.np_attention(hidden_out[:, :, :self.n_hidden].permute(1, 0, 2), hidden_out[-1, :, :self.n_hidden].unsqueeze(0))\n",
    "                np_attn_pool = torch.cat((np_attn_pool, self.np_attention(hidden_out[:, :, self.n_hidden:].permute(1, 0, 2), hidden_out[0, :, self.n_hidden:].unsqueeze(0))), dim=1)\n",
    "                attn_pool = self.attention(hidden_out, lengths=lengths)\n",
    "                last_pool = torch.cat((hidden_out[-1, :, :self.n_hidden], hidden_out[0, :, self.n_hidden:]), dim=1)\n",
    "            \n",
    "        pool_output = []\n",
    "        \n",
    "        if 'CNN' in self.hidden_type:\n",
    "            for p in self.pool_type:\n",
    "                if p == 'average':\n",
    "                    pool_output += avg_pool\n",
    "                elif p == 'max':\n",
    "                    pool_output += max_pool\n",
    "                elif p == 'attention':\n",
    "                    pool_output += attn_pool\n",
    "                    \n",
    "        else:\n",
    "            for p in self.pool_type:\n",
    "                if p == 'average':\n",
    "                    pool_output.append(avg_pool)\n",
    "                elif p == 'max':\n",
    "                    pool_output.append(max_pool)\n",
    "                elif p == 'np_attention':\n",
    "                    pool_output.append(np_attn_pool)\n",
    "                elif p == 'attention':\n",
    "                    pool_output.append(attn_pool)\n",
    "                elif p == 'last':\n",
    "                    pool_output.append(last_pool)\n",
    "           \n",
    "        pool_output = torch.cat(pool_output, dim=1)\n",
    "\n",
    "        if use_features:\n",
    "            feats_linear = self.linear(feats)\n",
    "            pool_output1 = torch.cat((pool_output, feats_linear), 1)\n",
    "            pool_output1 = self.dropout1(pool_output1)\n",
    "            outp = self.out(pool_output1)\n",
    "        else:\n",
    "            pool_output1 = self.dropout1(pool_output)\n",
    "            outp = self.out(pool_output1)\n",
    "  \n",
    "        return F.log_softmax(outp, dim=-1), pool_output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y4YFKKn219U6"
   },
   "outputs": [],
   "source": [
    "# load test set and extract linguistic features\n",
    "\n",
    "test = pd.read_csv('./data/test1.csv')\n",
    "\n",
    "with open('./features/distrib_features_from_vocab_bestfirst_greedystepwise_moydiff.txt') as f:\n",
    "    vocab = f.readlines()\n",
    "\n",
    "del vocab[18]\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    vocab[i] = vocab[i][:-3]\n",
    "\n",
    "    \n",
    "with open('./features/features_ngrams_bestfirst_greedystepwise.txt') as f:\n",
    "    tri = f.readlines()\n",
    "\n",
    "tri[-5] = 'à-l\\'eau-bouillante=1\\n'\n",
    "\n",
    "for i in range(len(tri)):\n",
    "    trigram = tri[i][:-3].replace('_',' ').replace('-', ' ')\n",
    "    tri[i] = trigram\n",
    "\n",
    "del tri[19]\n",
    "\n",
    "\n",
    "\n",
    "with open('./features/features_verbs.txt') as f:\n",
    "    verbs = f.readlines()\n",
    "\n",
    "del verbs[4]\n",
    "del verbs[13]\n",
    "\n",
    "verb_groups = [[], [], []]\n",
    "for i in range(len(verbs)):\n",
    "    idx = int(verbs[i][-2])-1\n",
    "    verb_groups[idx].append(verbs[i][:-3])\n",
    "      \n",
    "def featurize(dataset):\n",
    "    \n",
    "    x = {}\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        ID = dataset.ID[i]\n",
    "        features = np.zeros((4))\n",
    "        title = dataset.titre[i]\n",
    "        title = title.split()\n",
    "        features[0] = len(title)\n",
    "        \n",
    "        recipe = dataset.preparation[i]\n",
    "        if type(recipe) == float:\n",
    "            features[1] = 0\n",
    "        else:\n",
    "            recipe = recipe.split()\n",
    "            features[1] = len(recipe)\n",
    "\n",
    "        ing = ast.literal_eval(dataset.ingredients[i])\n",
    "        features[2] = len(ing)\n",
    "\n",
    "        price = dataset.cout[i]\n",
    "        if price == 'Bon marché':\n",
    "            features[3] = 1\n",
    "        elif price == 'Moyen':\n",
    "            features[3] = 2\n",
    "        elif price == 'Assez Cher':\n",
    "            features[3] = 3\n",
    "            \n",
    "        v = np.zeros((22))\n",
    "        if type(recipe) != float:\n",
    "            for j in range(len(vocab)):\n",
    "                if vocab[j] in recipe:\n",
    "                    v[j] = 1\n",
    "        features = np.append(features, v)\n",
    "        \n",
    "        t = np.zeros((48))\n",
    "        if type(recipe) != float:\n",
    "            for j in range(len(tri)):\n",
    "                if tri[j] in recipe:\n",
    "                    t[j] = 1\n",
    "        features = np.append(features, t)\n",
    "\n",
    "        vg = np.zeros((3))\n",
    "        if type(recipe) != float:\n",
    "            for j in range(len(verb_groups)):\n",
    "                num_verbs = 0\n",
    "                for k in range(len(verb_groups[j])):\n",
    "                    if verb_groups[j][k] in recipe:\n",
    "                        num_verbs += 1\n",
    "                vg[j] = num_verbs\n",
    "\n",
    "        features = np.append(features, vg)\n",
    "        \n",
    "        x[ID] = features\n",
    "        \n",
    "    return x\n",
    "  \n",
    "features = featurize(test)\n",
    "\n",
    "num_features = len(features[28300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c7lGsh-tzH9L"
   },
   "outputs": [],
   "source": [
    "# load and run saved trained models on test set\n",
    "\n",
    "files = os.listdir('./saved_models/')\n",
    "for file_name in tqdm_notebook(files):\n",
    "    with open('./saved_models/%s' % file_name, 'rb') as file:\n",
    "        msd, specs = pickle.load(file)\n",
    "    print(specs)\n",
    "  \n",
    "    embeddings = specs['embeddings']\n",
    "    hidden_type = specs['hidden_type']\n",
    "    n_hidden = specs['n_hidden']\n",
    "    n_layers = specs['n_layers']\n",
    "    pool_type = specs['pool_type']\n",
    "    direction = specs['direction']\n",
    "    embedding_dropout = specs['embedding_dropout']\n",
    "    dropout = specs['dropout']\n",
    "    batch_size = specs['batch_size']\n",
    "    maxlen = specs['maxlen']\n",
    "    n_out = specs['n_out']\n",
    "    emb_dim = specs['embedding_dim']\n",
    "    num_features = specs['num_features']\n",
    "    use_features = specs['use_features']\n",
    "\n",
    "\n",
    "    if gpu:\n",
    "        m = Model(emb_dim, hidden_type, n_hidden, n_layers, pool_type, n_out, direction, dropout, embedding_dropout).cuda()\n",
    "    else:\n",
    "        m = Model(emb_dim, hidden_type, n_hidden, n_layers, pool_type, n_out, direction, dropout, embedding_dropout)\n",
    "    m.load_state_dict(msd)\n",
    "    m.eval()\n",
    "  \n",
    "    with open('./embeddings/test_emb.pickle', 'rb') as handle:\n",
    "            test_data = pickle.load(handle)\n",
    "      \n",
    "    test_data = add_features(test, test_data, features)\n",
    "\n",
    "    test_dl = dataloader(test_data, batch_size, maxlen, emb_dim)\n",
    "  \n",
    "    test_userID = []\n",
    "    test_label = []\n",
    "    test_pred = []\n",
    "\n",
    "    for ids, feats, X, lengths, y1, y2 in tqdm_notebook(test_dl):\n",
    "        if gpu:\n",
    "            feats = feats.cuda()\n",
    "            X = Variable(X).cuda()\n",
    "            y = Variable(y1).type(torch.LongTensor).cuda()\n",
    "        else:\n",
    "            X = Variable(X)\n",
    "            y = Variable(y1).type(torch.LongTensor)\n",
    "        lengths = lengths.numpy()\n",
    "        pred, t_r = m(feats, X, lengths, gpu=gpu)\n",
    "        test_userID += list(ids.cpu().data.numpy())\n",
    "        test_label += list(y.cpu().data.numpy())\n",
    "        test_pred += list(np.e**pred.cpu().data.numpy())\n",
    "    \n",
    "    idx = np.argsort(test_userID)\n",
    "    test_userID = np.array([test_userID[i] for i in idx])\n",
    "    test_label = np.array([test_label[i] for i in idx])\n",
    "    test_pred = np.array([test_pred[i] for i in idx])\n",
    "\n",
    "    test_pred_idx = np.argmax(test_pred, axis=-1)\n",
    "    micro_f1 = f1_score(test_label, test_pred_idx, average='micro')\n",
    "    macro_precision = precision_score(test_label, test_pred_idx, average='macro')\n",
    "    macro_recall = recall_score(test_label, test_pred_idx, average='macro')\n",
    "    macro_f1 = f1_score(test_label, test_pred_idx, average='macro')\n",
    "    new_macro_f1 = (2*macro_precision*macro_recall)/(macro_precision+macro_recall)\n",
    "    cm_test = confusion_matrix(test_label, test_pred_idx)\n",
    "    f1_list = f1_score(test_label, test_pred_idx, average=None)\n",
    "\n",
    "    print('%s %s use_features: %s\\n' % (hidden_type, embeddings, use_features))\n",
    "    print('%s%s:\\Micro-f1 = %.1f - Macro-Precision = %.1f - Macro-Recall = %.1f - Macro-F1 = %.1f - New Macro-F1 = %.1f' % (hidden_type, embeddings, micro_f1*100, macro_precision*100, macro_recall*100, macro_f1*100, new_macro_f1*100))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(cm_test)\n",
    "    print('\\nF1 on Each Class:\\n')\n",
    "    print(f1_list)\n",
    "    print('mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Test_Task1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
