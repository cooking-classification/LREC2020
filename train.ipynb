{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Sco3BA59Y1d"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "\n",
    "import flair\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings\n",
    "from flair.embeddings import CharacterEmbeddings\n",
    "from flair.embeddings import StackedEmbeddings\n",
    "from flair.embeddings import FlairEmbeddings\n",
    "from flair.embeddings import BertEmbeddings\n",
    "from flair.embeddings import XLMEmbeddings\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1dqa4pr9_BdE"
   },
   "outputs": [],
   "source": [
    "# tools for the extraction of pretrained embeddings and linguistic features for each recipe, loading data, and calculation of metrics\n",
    "\n",
    "def initialize_embeddings(embeddings):\n",
    "    \"\"\"given a list of embedding names, initialize pretrained embeddings, stack them, and extract the dimension\"\"\"\n",
    "    \n",
    "    emb_list = []\n",
    "    for e in embeddings:\n",
    "        if e == 'fasttext':\n",
    "            fasttext_embedding = WordEmbeddings('fr')\n",
    "            emb_list.append(fasttext_embedding)\n",
    "        elif e == 'flair':\n",
    "            flair_forward  = FlairEmbeddings('fr-forward')\n",
    "            flair_backward = FlairEmbeddings('fr-backward')\n",
    "            emb_list.append(flair_forward)\n",
    "            emb_list.append(flair_backward)\n",
    "        elif e == 'xlm':\n",
    "            xlm_embedding = XLMEmbeddings('xlm-mlm-enfr-1024')\n",
    "            emb_list.append(xlm_embedding)\n",
    "        elif e == 'xlm-multi':\n",
    "            xlm_embedding_multi = XLMEmbeddings('xlm-mlm-tlm-xnli15-1024', pooling_operation='last')\n",
    "            emb_list.append(xlm_embedding_multi)\n",
    "        elif e == 'bert':\n",
    "            bert_embedding = BertEmbeddings('bert-base-multilingual-cased', layers='-1')\n",
    "            emb_list.append(bert_embedding)\n",
    "        elif e == 'camembert':\n",
    "            return None, 768\n",
    "\n",
    "    stacked_embeddings = StackedEmbeddings(embeddings = emb_list).eval()\n",
    "    s = Sentence('this is to extract embedding dimension!')\n",
    "    stacked_embeddings.embed(s)\n",
    "    emb_dim = len(s[0].embedding)\n",
    "    \n",
    "    return stacked_embeddings, emb_dim\n",
    "  \n",
    "  \n",
    "  \n",
    "def dataset_creator(data, stacked_embeddings, emb_dim, maxlen):\n",
    "    \"\"\"given dataset, initialized pretrained embeddings, and embedding dimension, output embedding vectors\"\"\"\n",
    "    \n",
    "    dataset = []\n",
    "    for i in tqdm_notebook(range(len(data))):   \n",
    "      # empty tensor for words #\n",
    "\n",
    "        sample = torch.zeros(0,emb_dim).cuda()\n",
    "            \n",
    "        text = ''\n",
    "        if type(data.titre[i]) == str:\n",
    "            text += data.titre[i]\n",
    "            text += ', '\n",
    "        if type(data.preparation[i]) == str:\n",
    "            text += data.preparation[i]\n",
    "        \n",
    "        if embeddings == ['camembert']:\n",
    "            limited = ' '.join(text.split(' ')[:maxlen])\n",
    "            tokens = camembert.encode(limited)\n",
    "            emb = camembert.extract_features(tokens).squeeze().cpu()\n",
    "            dataset.append((i, emb, emb.shape[0], data.niveau[i], data.plat[i]))\n",
    "\n",
    "        else:\n",
    "            sentence = Sentence(' '.join([str(tok).split(' ')[-1] for tok in Sentence(text)[:maxlen]]))\n",
    "            stacked_embeddings.embed(sentence)\n",
    "            # for every word #\n",
    "            for token in sentence:\n",
    "                # storing word Embeddings of each word in a sentence #\n",
    "                sample = torch.cat((sample,token.embedding.view(-1,emb_dim)),0)\n",
    "            dataset.append((i, sample, sample.shape[0], data.niveau[i], data.plat[i]))\n",
    "    \n",
    "    return dataset\n",
    "  \n",
    "  \n",
    "  \n",
    "def add_features(df, data, features):\n",
    "    \"\"\"given dataframe, embedding vectors, and a dictionary of features for samples, create samples that contain both embeddings and linguistic features\"\"\"\n",
    "    \n",
    "    dataset = []\n",
    "    for i in tqdm_notebook(range(len(data))):   \n",
    "      # empty tensor for words #\n",
    "\n",
    "        sample = data[i][1:]\n",
    "        idx = df.ID[i]\n",
    "        feat = torch.FloatTensor(features[idx])\n",
    "        new = (idx, feat, ) + sample\n",
    "\n",
    "        dataset.append(new)\n",
    "    \n",
    "    return dataset\n",
    "  \n",
    "  \n",
    "  \n",
    "def dataloader(data, batch_size, maxlen, emb_dim):\n",
    "    \"\"\"batch the vectorized data\"\"\"\n",
    "    \n",
    "    batches = []\n",
    "    \n",
    "    temp_data = deepcopy(data)\n",
    "    random.shuffle(temp_data)\n",
    "    \n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batches.append(temp_data[i:i + batch_size])\n",
    "    \n",
    "    dl = []\n",
    "    \n",
    "    for i in range(len(batches)):\n",
    "        batches[i] = sorted(batches[i], key=lambda x: x[3], reverse=True)\n",
    "        dl.append([])\n",
    "        \n",
    "        # IDs\n",
    "        dl[-1].append(torch.tensor([sample[0] for sample in batches[i]]))\n",
    "        \n",
    "        # linguistic features\n",
    "        dl[-1].append(torch.zeros(len(batches[i]), 77))\n",
    "        for j in range(len(batches[i])):\n",
    "            dl[-1][-1][j, :] = batches[i][j][1]\n",
    "        \n",
    "        # Embeddings\n",
    "        dl[-1].append(torch.zeros(len(batches[i]), min(maxlen, batches[i][0][3]), emb_dim))\n",
    "        for j in range(len(batches[i])):\n",
    "            dl[-1][-1][j, :min(maxlen, batches[i][j][3]), :] = batches[i][j][2][:min(maxlen, batches[i][j][3]), :]\n",
    "        dl[-1][-1] = dl[-1][-1].permute(1,0,2)\n",
    "        \n",
    "        # Lengths\n",
    "        dl[-1].append(torch.tensor([min(sample[3], maxlen) for sample in batches[i]]))\n",
    "        \n",
    "        # Labels\n",
    "        dl[-1].append(torch.tensor([sample[4] for sample in batches[i]]))\n",
    "        dl[-1].append(torch.tensor([sample[5] for sample in batches[i]]))\n",
    "        \n",
    "    return dl\n",
    "\n",
    "\n",
    "def average_metrics(true, pred):\n",
    "    \"\"\"given golden and predicted labels, output micro and macro scores, and f1 for each label\"\"\"\n",
    "    \n",
    "    micro_precision = precision_score(true, pred, average='micro')\n",
    "    micro_recall = recall_score(true, pred, average='micro')\n",
    "    micro_f1 = f1_score(true, pred, average='micro')\n",
    "    macro_precision = precision_score(true, pred, average='macro')\n",
    "    macro_recall = recall_score(true, pred, average='macro')\n",
    "    macro_f1 = f1_score(true, pred, average='macro')\n",
    "    new_macro_f1 = (2*macro_precision*macro_recall)/(macro_precision+macro_recall)\n",
    "    label_wise = f1_score(true, pred, average=None) \n",
    "\n",
    "    return(micro_precision, micro_recall, micro_f1, macro_precision, macro_recall, macro_f1, new_macro_f1, label_wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ppBzklNG_xWy"
   },
   "outputs": [],
   "source": [
    "# the joint model, composed of a neural and a linguistic sub-model\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_type, n_hidden, n_layers, pool_type, n_out, direction, dropout, emb_dropout):\n",
    "        super().__init__()\n",
    "        self.embedding_dim, self.hidden_type, self.n_hidden, self.n_layers, self.pool_type, self.n_out, self.direction, self.dropout, self.emb_dropout = emb_dim, hidden_type, n_hidden, n_layers, pool_type, n_out, direction, dropout, emb_dropout\n",
    "        self.embedding_dropout = nn.Dropout(p=self.emb_dropout)\n",
    "        if type(self.pool_type) != list:\n",
    "            self.pool_type = [self.pool_type]\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        if use_features:\n",
    "            self.linear = nn.Linear(num_features, num_features)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        if self.hidden_type == 'GRU-CNN':\n",
    "            self.number_of_hidden = 64\n",
    "            self.number_of_layers = 1\n",
    "          \n",
    "            self.hidden1 = nn.GRU(input_size=self.embedding_dim, hidden_size=self.number_of_hidden, num_layers=self.number_of_layers, dropout = self.dropout, bidirectional = True)\n",
    "            self.hidden = []\n",
    "            for i in range(len(self.n_hidden)):\n",
    "                self.hidden.append(nn.Conv2d(in_channels=1, out_channels=self.n_hidden[i], kernel_size=(self.n_layers[i], 2*self.number_of_hidden), padding=(self.n_layers[i]//2, 0)))\n",
    "            self.hidden = nn.ModuleList(self.hidden)\n",
    "            self.attn = []\n",
    "            for i in range(len(self.n_hidden)):\n",
    "                self.attn.append(nn.Linear(self.n_hidden[i], 1))\n",
    "            self.attn = nn.ModuleList(self.attn)\n",
    "            if 'max' in pool_type:\n",
    "                if use_features:\n",
    "                    self.out = nn.Linear((len(self.pool_type)+1)*sum(self.n_hidden)+num_features, self.n_out)\n",
    "                else:\n",
    "                    self.out = nn.Linear((len(self.pool_type)+1)*sum(self.n_hidden), self.n_out)\n",
    "            else:\n",
    "                if use_features:\n",
    "                    self.out = nn.Linear(len(self.pool_type)*sum(self.n_hidden)+num_features, self.n_out)\n",
    "                else:\n",
    "                    self.out = nn.Linear(len(self.pool_type)*sum(self.n_hidden), self.n_out)\n",
    "\n",
    "        elif self.hidden_type == 'transformer':\n",
    "            self.hidden = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=n_hidden, nhead=6), num_layers=n_layers)\n",
    "            self.attn = nn.Linear(self.n_hidden, 1)\n",
    "            if 'max' in pool_type:\n",
    "                if use_features:\n",
    "                    self.out = nn.Linear((len(self.pool_type)+1) * self.n_hidden+num_features, self.n_out)\n",
    "                else:\n",
    "                    self.out = nn.Linear((len(self.pool_type)+1) * self.n_hidden, self.n_out)\n",
    "            else:\n",
    "                if use_features:\n",
    "                    self.out = nn.Linear(len(self.pool_type) * self.n_hidden+num_features, self.n_out)\n",
    "                else:\n",
    "                    self.out = nn.Linear(len(self.pool_type) * self.n_hidden, self.n_out)\n",
    "        \n",
    "        elif self.hidden_type == 'CNN':\n",
    "            self.hidden = []\n",
    "            for i in range(len(self.n_hidden)):\n",
    "                self.hidden.append(nn.Conv2d(in_channels=1, out_channels=self.n_hidden[i], kernel_size=(self.n_layers[i], self.embedding_dim), padding=(self.n_layers[i]//2, 0)))\n",
    "            self.hidden = nn.ModuleList(self.hidden)\n",
    "            self.attn = []\n",
    "            for i in range(len(self.n_hidden)):\n",
    "                self.attn.append(nn.Linear(self.n_hidden[i], 1))\n",
    "            self.attn = nn.ModuleList(self.attn)\n",
    "            if 'max' in pool_type:\n",
    "                if use_features:\n",
    "                    self.out = nn.Linear((len(self.pool_type)+1)*sum(self.n_hidden)+num_features, self.n_out)\n",
    "                else:\n",
    "                    self.out = nn.Linear((len(self.pool_type)+1)*sum(self.n_hidden), self.n_out)\n",
    "            else:\n",
    "                if use_features:\n",
    "                    self.out = nn.Linear(len(self.pool_type)*sum(self.n_hidden)+num_features, self.n_out)\n",
    "                else:\n",
    "                    self.out = nn.Linear(len(self.pool_type)*sum(self.n_hidden), self.n_out)\n",
    "  \n",
    "        elif self.direction == 'unidirectional':\n",
    "            if self.hidden_type == 'RNN':\n",
    "                self.hidden = nn.RNN(input_size=self.embedding_dim, hidden_size=self.n_hidden, num_layers=self.n_layers, dropout = self.dropout)\n",
    "            elif self.hidden_type == 'GRU':\n",
    "                self.hidden = nn.GRU(input_size=self.embedding_dim, hidden_size=self.n_hidden, num_layers=self.n_layers, dropout = self.dropout)\n",
    "            elif self.hidden_type == 'LSTM':\n",
    "                self.hidden = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.n_hidden, num_layers=self.n_layers, dropout = self.dropout)\n",
    "            self.attn = nn.Linear(self.n_hidden, 1)\n",
    "            if 'max' in pool_type:\n",
    "                if use_features:\n",
    "                    self.out = nn.Linear((len(self.pool_type)+1) * self.n_hidden+num_features, self.n_out)\n",
    "                else:\n",
    "                    self.out = nn.Linear((len(self.pool_type)+1) * self.n_hidden, self.n_out)\n",
    "            else:\n",
    "                if use_features:\n",
    "                    self.out = nn.Linear(len(self.pool_type) * self.n_hidden+num_features, self.n_out)\n",
    "                else:\n",
    "                    self.out = nn.Linear(len(self.pool_type) * self.n_hidden, self.n_out)\n",
    "\n",
    "        elif self.direction == 'bidirectional':\n",
    "            if self.hidden_type == 'RNN':\n",
    "                self.hidden = nn.RNN(input_size=self.embedding_dim, hidden_size=self.n_hidden, num_layers=self.n_layers, dropout = self.dropout, bidirectional = True)\n",
    "            elif self.hidden_type == 'GRU':\n",
    "                self.hidden = nn.GRU(input_size=self.embedding_dim, hidden_size=self.n_hidden, num_layers=self.n_layers, dropout = self.dropout, bidirectional = True)\n",
    "            elif self.hidden_type == 'LSTM':\n",
    "                self.hidden = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.n_hidden, num_layers=self.n_layers, dropout = self.dropout, bidirectional = True)\n",
    "                         \n",
    "            self.attn = nn.Linear(2*self.n_hidden, 1)\n",
    "            if 'max' in pool_type:\n",
    "                if use_features:\n",
    "                    self.out = nn.Linear(2 * (len(self.pool_type)+1) * self.n_hidden+num_features, self.n_out)\n",
    "                else:\n",
    "                    self.out = nn.Linear(2 * (len(self.pool_type)+1) * self.n_hidden, self.n_out)\n",
    "            else:\n",
    "                if use_features:\n",
    "                    self.out = nn.Linear(2 * len(self.pool_type) * self.n_hidden+num_features, self.n_out)\n",
    "                else:\n",
    "                    self.out = nn.Linear(2 * len(self.pool_type) * self.n_hidden, self.n_out)\n",
    "    \n",
    "    def deactivate_dropout(self):\n",
    "        self.embedding_dropout = nn.Dropout(p=0)\n",
    "        \n",
    "    def activate_dropout(self):\n",
    "        self.embedding_dropout = nn.Dropout(p=self.emb_dropout)\n",
    "    \n",
    "    def init_hidden(self, batch_size, gpu = False):\n",
    "        \n",
    "        if self.hidden_type == 'GRU-CNN':\n",
    "            if gpu:\n",
    "                return Variable(torch.zeros(2 * self.number_of_layers,batch_size,self.number_of_hidden).cuda())\n",
    "            else:\n",
    "                return Variable(torch.zeros(2 * self.number_of_layers,batch_size,self.number_of_hidden))\n",
    "          \n",
    "        if self.direction == 'unidirectional':\n",
    "            if gpu:\n",
    "                return Variable(torch.zeros(self.n_layers,batch_size,self.n_hidden).cuda())\n",
    "            else:\n",
    "                return Variable(torch.zeros(self.n_layers,batch_size,self.n_hidden))\n",
    "        elif self.direction == 'bidirectional':\n",
    "            if gpu:\n",
    "                return Variable(torch.zeros(2 * self.n_layers,batch_size,self.n_hidden).cuda())\n",
    "            else:\n",
    "                return Variable(torch.zeros(2 * self.n_layers,batch_size,self.n_hidden))\n",
    "    \n",
    "    def attention(self, hidden_out, lengths=None, n=None):\n",
    "        if n==None:\n",
    "            attn_out = self.attn(hidden_out)\n",
    "        else:\n",
    "            attn_out = self.attn[n](hidden_out)\n",
    "        if lengths is not None:\n",
    "            mask = torch.arange(torch.tensor(lengths[0]))[None, :] >= torch.tensor(lengths[:, None])\n",
    "            attn_out[mask.permute(1,0)] = float('-inf')\n",
    "        attn_weights = F.softmax(attn_out, 0)\n",
    "        new_hidden_state = torch.sum(hidden_out * attn_weights, 0)\n",
    "        return new_hidden_state\n",
    "    \n",
    "    def np_attention(self, hidden_out, h):\n",
    "        hidden = h.squeeze(0)\n",
    "        attn_weights = torch.bmm(hidden_out, hidden.unsqueeze(2)).squeeze(2)\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        new_hidden_state = torch.bmm(hidden_out.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        return new_hidden_state\n",
    "    \n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3)  # (N, Co, W)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, feats, seq, lengths, gpu = False):\n",
    "        bs = seq.size(1) # batch size\n",
    "        embs = seq\n",
    "        \n",
    "        if self.hidden_type != 'CNN' and self.hidden_type != 'transformer':\n",
    "            embs = pack_padded_sequence(embs, lengths)\n",
    "            self.h = self.init_hidden(bs, gpu)\n",
    "            self.c = self.init_hidden(bs, gpu)\n",
    "            \n",
    "        if self.hidden_type == 'RNN':\n",
    "            hidden_out, self.h = self.hidden(embs, self.h)\n",
    "        elif self.hidden_type == 'GRU':\n",
    "            hidden_out, self.h = self.hidden(embs, self.h)\n",
    "        elif self.hidden_type == 'LSTM':\n",
    "            hidden_out, (self.h, self.c) = self.hidden(embs, (self.h, self.c))\n",
    "        elif self.hidden_type == 'CNN':\n",
    "            hidden_out = []\n",
    "            for cnn in self.hidden:\n",
    "                hidden_out.append(cnn(embs.permute(1, 0, 2).unsqueeze(1)).squeeze(-1).permute(2, 0, 1))\n",
    "        elif self.hidden_type == 'GRU-CNN':\n",
    "            h_out, self.h = self.hidden1(embs, self.h)\n",
    "            h_out, lengths = pad_packed_sequence(h_out)\n",
    "            hidden_out = []\n",
    "            for cnn in self.hidden:\n",
    "                hidden_out.append(cnn(h_out.permute(1, 0, 2).unsqueeze(1)).squeeze(-1).permute(2, 0, 1))\n",
    "            \n",
    "        if 'CNN' in self.hidden_type:\n",
    "            avg_pool, max_pool, attn_pool = [], [], []\n",
    "            for i in range(len(hidden_out)):\n",
    "                avg_pool.append(F.adaptive_avg_pool1d(hidden_out[i].permute(1,2,0),1).view(seq.size(1),-1))\n",
    "                max_pool.append(F.adaptive_max_pool1d(self.relu(torch.cat([hidden_out[i], -hidden_out[i]], dim=-1)).permute(1,2,0),1).view(seq.size(1),-1))\n",
    "                attn_pool.append(self.attention(hidden_out[i], n=i))\n",
    "                \n",
    "        elif self.hidden_type == 'transformer':\n",
    "            padding_mask = torch.arange(torch.tensor(lengths[0]))[None, :] >= torch.tensor(lengths[:, None])\n",
    "            if gpu:\n",
    "                padding_mask = padding_mask.cuda()\n",
    "            hidden_out = self.hidden(embs, src_key_padding_mask=padding_mask)\n",
    "            avg_pool = F.adaptive_avg_pool1d(hidden_out.permute(1,2,0),1).view(seq.size(1),-1)\n",
    "            max_pool = F.adaptive_max_pool1d(self.relu(torch.cat([hidden_out, -hidden_out], dim=-1)).permute(1,2,0),1).view(seq.size(1),-1)\n",
    "            attn_pool = self.attention(hidden_out, lengths=lengths)\n",
    "        \n",
    "        else:\n",
    "            hidden_out, lengths = pad_packed_sequence(hidden_out)\n",
    "        \n",
    "            if self.direction == 'unidirectional':\n",
    "                avg_pool = F.adaptive_avg_pool1d(hidden_out.permute(1,2,0),1).view(seq.size(1),-1)\n",
    "                max_pool = F.adaptive_max_pool1d(self.relu(torch.cat([hidden_out, -hidden_out], dim=-1)).permute(1,2,0),1).view(seq.size(1),-1)\n",
    "                np_attn_pool = self.np_attention(hidden_out.permute(1, 0, 2), hidden_out[-1].unsqueeze(0))\n",
    "                attn_pool = self.attention(hidden_out, lengths=lengths)\n",
    "                last_pool = hidden_out[-1]\n",
    "            elif self.direction == 'bidirectional':\n",
    "                avg_pool = F.adaptive_avg_pool1d(hidden_out.permute(1,2,0),1).view(seq.size(1),-1)\n",
    "                max_pool = F.adaptive_max_pool1d(self.relu(torch.cat([hidden_out, -hidden_out], dim=-1)).permute(1,2,0),1).view(seq.size(1),-1)\n",
    "                np_attn_pool = self.np_attention(hidden_out[:, :, :self.n_hidden].permute(1, 0, 2), hidden_out[-1, :, :self.n_hidden].unsqueeze(0))\n",
    "                np_attn_pool = torch.cat((np_attn_pool, self.np_attention(hidden_out[:, :, self.n_hidden:].permute(1, 0, 2), hidden_out[0, :, self.n_hidden:].unsqueeze(0))), dim=1)\n",
    "                attn_pool = self.attention(hidden_out, lengths=lengths)\n",
    "                last_pool = torch.cat((hidden_out[-1, :, :self.n_hidden], hidden_out[0, :, self.n_hidden:]), dim=1)\n",
    "            \n",
    "        pool_output = []\n",
    "        \n",
    "        if 'CNN' in self.hidden_type:\n",
    "            for p in self.pool_type:\n",
    "                if p == 'average':\n",
    "                    pool_output += avg_pool\n",
    "                elif p == 'max':\n",
    "                    pool_output += max_pool\n",
    "                elif p == 'attention':\n",
    "                    pool_output += attn_pool\n",
    "                    \n",
    "        else:\n",
    "            for p in self.pool_type:\n",
    "                if p == 'average':\n",
    "                    pool_output.append(avg_pool)\n",
    "                elif p == 'max':\n",
    "                    pool_output.append(max_pool)\n",
    "                elif p == 'np_attention':\n",
    "                    pool_output.append(np_attn_pool)\n",
    "                elif p == 'attention':\n",
    "                    pool_output.append(attn_pool)\n",
    "                elif p == 'last':\n",
    "                    pool_output.append(last_pool)\n",
    "           \n",
    "        pool_output = torch.cat(pool_output, dim=1)\n",
    "\n",
    "        if use_features:\n",
    "            feats_linear = self.linear(feats)\n",
    "            pool_output1 = torch.cat((pool_output, feats_linear), 1)\n",
    "            pool_output1 = self.dropout1(pool_output1)\n",
    "            outp = self.out(pool_output1)\n",
    "        else:\n",
    "            pool_output1 = self.dropout1(pool_output)\n",
    "            outp = self.out(pool_output1)\n",
    "  \n",
    "        return F.log_softmax(outp, dim=-1), pool_output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OPuYmRVrT7_s"
   },
   "outputs": [],
   "source": [
    "def adjust_lr(learning_rate, opt, epoch):\n",
    "\n",
    "    \"\"\"update the learning rate using learning rate decay\"\"\"\n",
    "    if epoch < 3:\n",
    "        lr = learning_rate\n",
    "    else:\n",
    "        lr = 1e-4\n",
    "      \n",
    "    for param_group in opt.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def fit(specs, model, train_data, dev_data, loss_fn, opt, epochs=3, batch_size = 1):\n",
    "    \n",
    "    name = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "#     with open('./path/to/results/folder/' + name + '.txt', 'w') as text_file:\n",
    "#                 for i in specs:\n",
    "#                     text_file.write(str(i)+' = '+str(specs[i])+'\\n')\n",
    "#                 text_file.write(\"\\n\")\n",
    "    \n",
    "    max_f1 = 0\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(epochs)):\n",
    "        \n",
    "        adjust_lr(learning_rate, opt, epoch)\n",
    "        \n",
    "        train_dl = dataloader(train_data, batch_size, maxlen, emb_dim)\n",
    "        num_batch = len(train_dl)\n",
    "        y_true_train = list()\n",
    "        y_pred_train = list()\n",
    "        total_loss_train = 0\n",
    "        \n",
    "        y_true_dev = list()\n",
    "        y_pred_dev = list()\n",
    "        total_loss_dev = 0\n",
    "\n",
    "        model = model.train()\n",
    "        \n",
    "        t = tqdm_notebook(iter(train_dl), leave=False, total=num_batch)\n",
    "        for ids, feats, X, lengths, y1, y2 in t:\n",
    "            t.set_description(f'Epoch {epoch}')\n",
    "            \n",
    "            if gpu:\n",
    "                feats = feats.cuda()\n",
    "                X = Variable(X).cuda()\n",
    "                y = Variable(y1).type(torch.LongTensor).cuda()\n",
    "            else:\n",
    "                X = Variable(X)\n",
    "                y = Variable(y1).type(torch.LongTensor)\n",
    "\n",
    "            lengths = lengths.numpy()\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            pred, _ = model(feats, X, lengths, gpu=gpu)\n",
    "            loss = loss_fn(pred, y, weight=train_weights)\n",
    "            loss.backward()\n",
    "            if clipping == True:\n",
    "                clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "            \n",
    "            opt.step()\n",
    "            \n",
    "            t.set_postfix(loss=loss.data)\n",
    "            pred_idx = torch.max(pred, dim=1)[1]\n",
    "            y_true_train.append(y.cpu().data.numpy())\n",
    "            y_pred_train.append(pred_idx.cpu().data.numpy())\n",
    "            total_loss_train += loss.data\n",
    "\n",
    "        y_true_train = np.concatenate(y_true_train, axis=0)\n",
    "        y_pred_train = np.concatenate(y_pred_train, axis=0)\n",
    "\n",
    "        train_micro_precision, train_micro_recall, train_micro_f1, train_macro_precision, train_macro_recall, train_macro_f1, train_new_macro_f1, train_list = average_metrics(y_true_train, y_pred_train)\n",
    "        train_loss = total_loss_train/len(train_dl)\n",
    "        \n",
    "        #dev phase begins\n",
    "        model.eval()\n",
    "        dev_dl = dataloader(dev_data, batch_size, maxlen, emb_dim)\n",
    "        for ids, feats, X, lengths, y1, y2 in tqdm_notebook(dev_dl, leave = False):\n",
    "          \n",
    "            if gpu:\n",
    "                feats = feats.cuda()\n",
    "                X = Variable(X).cuda()\n",
    "                y = Variable(y1).type(torch.LongTensor).cuda()\n",
    "            else:\n",
    "                X = Variable(X)\n",
    "                y = Variable(y1).type(torch.LongTensor)\n",
    "              \n",
    "            pred, _ = model(feats, X, lengths.numpy(), gpu = gpu)\n",
    "            loss = loss_fn(pred, y, weight=train_weights)\n",
    "            pred_idx = torch.max(pred, dim=1)[1]\n",
    "            y_true_dev.append(y.cpu().data.numpy())\n",
    "            y_pred_dev.append(pred_idx.cpu().data.numpy())\n",
    "            total_loss_dev += loss.data\n",
    "\n",
    "        y_true_dev = np.concatenate(y_true_dev, axis=0)\n",
    "        y_pred_dev = np.concatenate(y_pred_dev, axis=0)\n",
    "        \n",
    "        dev_micro_precision, dev_micro_recall, dev_micro_f1, dev_macro_precision, dev_macro_recall, dev_macro_f1, dev_new_macro_f1, dev_list = average_metrics(y_true_dev, y_pred_dev)\n",
    "        dev_loss = total_loss_dev/len(dev_dl)\n",
    "\n",
    "        report_string = '-----------------------------------------------------------\\n'\n",
    "        report_string += 'Epoch = %d\\n\\nTrain:\\tLoss = %.3f - Mic_Precision = %.3f - Mic_Recall = %.3f - Mic_F1 = %.3f - Mac_Precision = %.3f - Mac_Recall = %.3f - Mac_F1 = %.3f - New_Mac_F1 = %.3f\\n\\n' % \\\n",
    "            (epoch+1, train_loss*100, train_micro_precision*100, train_micro_recall*100, train_micro_f1*100, train_macro_precision*100, train_macro_recall*100, train_macro_f1*100, train_new_macro_f1*100)\n",
    "        \n",
    "        report_string += 'Dev:\\tLoss = %.3f - Mic_Precision = %.3f - Mic_Recall = %.3f - Mic_F1 = %.3f - Mac_Precision = %.3f - Mac_Recall = %.3f - Mac_F1 = %.3f - New_Mac_F1 = %.3f\\n\\n' % \\\n",
    "            (dev_loss*100, dev_micro_precision*100, dev_micro_recall*100, dev_micro_f1*100, dev_macro_precision*100, dev_macro_recall*100, dev_macro_f1*100, dev_new_macro_f1*100)\n",
    "        \n",
    "        print(report_string, end='')\n",
    "        print('Per Class F1 on Dev:\\n')\n",
    "        print(dev_list)\n",
    "#         with open('./path/to/results/folder/' + name + \".txt\", \"a\") as text_file:\n",
    "#             text_file.write(report_string)\n",
    "            \n",
    "        if dev_micro_f1 > max_f1:\n",
    "            max_f1 = dev_micro_f1\n",
    "            with open('../saved_models/%s-%s-%s-%s.pkl' % (str(specs['embeddings']), str(specs['hidden_type']), str(specs['pool_type']), str(specs['use_features'])), 'wb') as file:\n",
    "                pickle.dump((model.state_dict(), specs), file)\n",
    "            print('\\nmax_f1 overwritten (%.3f)!' % (max_f1*100))\n",
    "        else:\n",
    "            print('\\nmax_f1 kept the same (%.3f)!' % (max_f1*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3E0EKqLD9ZBx"
   },
   "outputs": [],
   "source": [
    "# load datasets\n",
    "\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "dev = pd.read_csv('./data/dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D4jEH6Qq9x5T"
   },
   "outputs": [],
   "source": [
    "# define hyper-parameters\n",
    "# if you choose CNN as hidden_type, n_hidden is number of filters and n_layers is filter size. Both should be input as list.\n",
    "\n",
    "embeddings = ['bert']\n",
    "hidden_type = 'CNN'\n",
    "n_hidden = [250, 50]\n",
    "n_layers = [2, 3]\n",
    "pool_type = ['max']\n",
    "direction = 'bidirectional'\n",
    "embedding_dropout = 0\n",
    "dropout = 0.2\n",
    "clipping = True\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "weight_decay = 0\n",
    "maxlen = 100\n",
    "requires_grad = False\n",
    "n_out = 4\n",
    "\n",
    "use_features = False\n",
    "gpu = True\n",
    "\n",
    "if 'camembert' in embeddings:\n",
    "    camembert = torch.hub.load('pytorch/fairseq', 'camembert.v0')\n",
    "    camembert.eval()\n",
    "    for param in camembert.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8KrOz032AJh2"
   },
   "outputs": [],
   "source": [
    "#initialize embeddings\n",
    "\n",
    "stacked_embeddings, emb_dim = initialize_embeddings(embeddings)\n",
    "print(emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UpA_HGw4I5lo"
   },
   "outputs": [],
   "source": [
    "# load already saved embeddings\n",
    "\n",
    "with open('./embeddings/train_emb.pickle', 'rb') as handle:\n",
    "    train_data = pickle.load(handle)\n",
    "\n",
    "with open('./embeddings/dev_emb.pickle', 'rb') as handle:\n",
    "    dev_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lnJ8jwUw5NgK"
   },
   "outputs": [],
   "source": [
    "# extract linguistic features\n",
    "\n",
    "with open('./features/distrib_features_from_vocab_bestfirst_greedystepwise_moydiff.txt') as f:\n",
    "    vocab = f.readlines()\n",
    "\n",
    "del vocab[18]\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    vocab[i] = vocab[i][:-3]\n",
    "\n",
    "    \n",
    "with open('./features/features_ngrams_bestfirst_greedystepwise.txt') as f:\n",
    "    tri = f.readlines()\n",
    "\n",
    "tri[-5] = 'à-l\\'eau-bouillante=1\\n'\n",
    "\n",
    "for i in range(len(tri)):\n",
    "    trigram = tri[i][:-3].replace('_',' ').replace('-', ' ')\n",
    "    tri[i] = trigram\n",
    "\n",
    "del tri[19]\n",
    "\n",
    "\n",
    "\n",
    "with open('./features/features_verbs.txt') as f:\n",
    "    verbs = f.readlines()\n",
    "\n",
    "del verbs[4]\n",
    "del verbs[13]\n",
    "\n",
    "verb_groups = [[], [], []]\n",
    "for i in range(len(verbs)):\n",
    "    idx = int(verbs[i][-2])-1\n",
    "    verb_groups[idx].append(verbs[i][:-3])\n",
    "      \n",
    "\n",
    "def featurize(dataset):\n",
    "    \n",
    "    x = {}\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        ID = dataset.ID[i]\n",
    "        features = np.zeros((4))\n",
    "        title = dataset.titre[i]\n",
    "        title = title.split()\n",
    "        features[0] = len(title)\n",
    "        \n",
    "        recipe = dataset.preparation[i]\n",
    "        if type(recipe) == float:\n",
    "            features[1] = 0\n",
    "        else:\n",
    "            recipe = recipe.split()\n",
    "            features[1] = len(recipe)\n",
    "\n",
    "        ing = ast.literal_eval(dataset.ingredients[i])\n",
    "        features[2] = len(ing)\n",
    "\n",
    "        price = dataset.cout[i]\n",
    "        if price == 'Bon marché':\n",
    "            features[3] = 1\n",
    "        elif price == 'Moyen':\n",
    "            features[3] = 2\n",
    "        elif price == 'Assez Cher':\n",
    "            features[3] = 3\n",
    "            \n",
    "        v = np.zeros((22))\n",
    "        if type(recipe) != float:\n",
    "            for j in range(len(vocab)):\n",
    "                if vocab[j] in recipe:\n",
    "                    v[j] = 1\n",
    "        features = np.append(features, v)\n",
    "        \n",
    "        t = np.zeros((48))\n",
    "        if type(recipe) != float:\n",
    "            for j in range(len(tri)):\n",
    "                if tri[j] in recipe:\n",
    "                    t[j] = 1\n",
    "        features = np.append(features, t)\n",
    "\n",
    "        vg = np.zeros((3))\n",
    "        if type(recipe) != float:\n",
    "            for j in range(len(verb_groups)):\n",
    "                num_verbs = 0\n",
    "                for k in range(len(verb_groups[j])):\n",
    "                    if verb_groups[j][k] in recipe:\n",
    "                        num_verbs += 1\n",
    "                vg[j] = num_verbs\n",
    "\n",
    "        features = np.append(features, vg)\n",
    "        \n",
    "        x[ID] = features\n",
    "        \n",
    "    return x\n",
    "  \n",
    "\n",
    "train_features = featurize(train)\n",
    "dev_features = featurize(dev)\n",
    "\n",
    "features = {**train_features, **dev_features}\n",
    "num_features = len(features[50819])\n",
    "\n",
    "# add the linguistc features to the pretrained embeddings that were loaded\n",
    "train_data = add_features(train, train_data, features)\n",
    "dev_data = add_features(dev, dev_data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bK4ia8WEQvX9"
   },
   "outputs": [],
   "source": [
    "# define weights for different classes to counter class imbalance\n",
    "\n",
    "if use_features:\n",
    "    train_weights = torch.FloatTensor([0.1, 0.1, 0.2, 0.6])\n",
    "else:\n",
    "    train_weights = np.bincount([t for t in train.niveau])[0] / np.bincount([t for t in train.niveau])\n",
    "    train_weights = torch.FloatTensor(np.divide(train_weights, np.sum(train_weights)))\n",
    "    \n",
    "if gpu == True:\n",
    "    train_weights = train_weights.cuda()   \n",
    "train_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LsrageYjB3Q6"
   },
   "outputs": [],
   "source": [
    "# start training using the hyper-parameters defined above\n",
    "\n",
    "specs = {'embeddings': embeddings, 'embedding_dim': emb_dim, 'hidden_type': hidden_type,\n",
    "         'n_hidden': n_hidden, 'n_layers': n_layers, 'pool_type': pool_type,\n",
    "         'n_out': n_out, 'direction': direction, 'dropout': dropout, 'embedding_dropout': embedding_dropout, 'maxlen': maxlen,\n",
    "         'clipping': clipping, 'batch_size': batch_size, 'learning_rate': learning_rate, 'weight_decay': weight_decay, 'requires_grad': requires_grad, 'train_weights': train_weights, 'num_features': num_features, 'use_features' : use_features}\n",
    "\n",
    "if gpu:\n",
    "    m = Model(emb_dim, hidden_type, n_hidden, n_layers, pool_type, n_out, direction, dropout, embedding_dropout).cuda()\n",
    "else:\n",
    "    m = Model(emb_dim, hidden_type, n_hidden, n_layers, pool_type, n_out, direction, dropout, embedding_dropout)\n",
    "    \n",
    "opt = optim.AdamW(filter(lambda p: p.requires_grad,m.parameters()), learning_rate, weight_decay=weight_decay)\n",
    "fit(specs, model=m, train_data=train_data, dev_data=dev_data, loss_fn=F.nll_loss, opt=opt, epochs=80, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Train_Task1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
