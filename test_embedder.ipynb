{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Sco3BA59Y1d"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import flair\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings\n",
    "from flair.embeddings import CharacterEmbeddings\n",
    "from flair.embeddings import StackedEmbeddings\n",
    "from flair.embeddings import FlairEmbeddings\n",
    "from flair.embeddings import BertEmbeddings\n",
    "from flair.embeddings import XLMEmbeddings\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1dqa4pr9_BdE"
   },
   "outputs": [],
   "source": [
    "# tools for the extraction of pretrained embeddings for each recipe\n",
    "\n",
    "def initialize_embeddings(embeddings):\n",
    "    \"\"\"given a list of embedding names, initialize pretrained embeddings, stack them, and extract the dimension\"\"\"\n",
    "    \n",
    "    emb_list = []\n",
    "    for e in embeddings:\n",
    "        if e == 'fasttext':\n",
    "            fasttext_embedding = WordEmbeddings('fr')\n",
    "            emb_list.append(fasttext_embedding)\n",
    "        elif e == 'flair':\n",
    "            flair_forward  = FlairEmbeddings('fr-forward')\n",
    "            flair_backward = FlairEmbeddings('fr-backward')\n",
    "            emb_list.append(flair_forward)\n",
    "            emb_list.append(flair_backward)\n",
    "        elif e == 'xlm':\n",
    "            xlm_embedding = XLMEmbeddings('xlm-mlm-enfr-1024')\n",
    "            emb_list.append(xlm_embedding)\n",
    "        elif e == 'xlm-multi':\n",
    "            xlm_embedding_multi = XLMEmbeddings('xlm-mlm-tlm-xnli15-1024', pooling_operation='last')\n",
    "            emb_list.append(xlm_embedding_multi)\n",
    "        elif e == 'bert':\n",
    "            bert_embedding = BertEmbeddings('bert-base-multilingual-cased', layers='-1')\n",
    "            emb_list.append(bert_embedding)\n",
    "        elif e == 'camembert':\n",
    "            return None, 768\n",
    "\n",
    "    stacked_embeddings = StackedEmbeddings(embeddings = emb_list).eval()\n",
    "    s = Sentence('this is to extract embedding dimension!')\n",
    "    stacked_embeddings.embed(s)\n",
    "    emb_dim = len(s[0].embedding)\n",
    "    \n",
    "    return stacked_embeddings, emb_dim\n",
    "  \n",
    "  \n",
    "  \n",
    "def dataset_creator(data, stacked_embeddings, emb_dim, maxlen):\n",
    "    \"\"\"given dataset, initialized pretrained embeddings, and embedding dimension, output embedding vectors\"\"\"\n",
    "    \n",
    "    dataset = []\n",
    "    for i in tqdm_notebook(range(len(data))):   \n",
    "      # empty tensor for words #\n",
    "\n",
    "        sample = torch.zeros(0,emb_dim).cuda()\n",
    "            \n",
    "        text = ''\n",
    "        if type(data.titre[i]) == str:\n",
    "            text += data.titre[i]\n",
    "            text += ', '\n",
    "        if type(data.preparation[i]) == str:\n",
    "            text += data.preparation[i]\n",
    "        \n",
    "        if embeddings == ['camembert']:\n",
    "            limited = ' '.join(text.split(' ')[:maxlen])\n",
    "            tokens = camembert.encode(limited)\n",
    "            emb = camembert.extract_features(tokens).squeeze().cpu()\n",
    "            dataset.append((i, emb, emb.shape[0], data.niveau[i], data.plat[i]))\n",
    "\n",
    "        else:\n",
    "            sentence = Sentence(' '.join([str(tok).split(' ')[-1] for tok in Sentence(text)[:maxlen]]))\n",
    "            stacked_embeddings.embed(sentence)\n",
    "            # for every word #\n",
    "            for token in sentence:\n",
    "                # storing word Embeddings of each word in a sentence #\n",
    "                sample = torch.cat((sample,token.embedding.view(-1,emb_dim)),0)\n",
    "            dataset.append((i, sample, sample.shape[0], data.niveau[i], data.plat[i]))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3E0EKqLD9ZBx"
   },
   "outputs": [],
   "source": [
    "# load test set\n",
    "\n",
    "test = pd.read_csv('/content/gdrive/My Drive/LREC/data/test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D4jEH6Qq9x5T"
   },
   "outputs": [],
   "source": [
    "# define embedding type and maximum number of words here\n",
    "embeddings = ['bert']\n",
    "maxlen = 100\n",
    "\n",
    "if 'camembert' in embeddings:\n",
    "    camembert = torch.hub.load('pytorch/fairseq', 'camembert.v0')\n",
    "    camembert.eval()\n",
    "    for param in camembert.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8KrOz032AJh2"
   },
   "outputs": [],
   "source": [
    "#initialize embeddings\n",
    "\n",
    "stacked_embeddings, emb_dim = initialize_embeddings(embeddings)\n",
    "print(emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KePo8YOCrDK1"
   },
   "outputs": [],
   "source": [
    "# vectorize data and save embeddings in 'embeddings' folder\n",
    "\n",
    "if not os.path.exists('./embeddings'):\n",
    "    os.mkdir('./embeddings')\n",
    "\n",
    "test_data = dataset_creator(test, stacked_embeddings, emb_dim, maxlen)\n",
    "\n",
    "with open('./embeddings/test_emb.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_data, handle)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "embedder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
